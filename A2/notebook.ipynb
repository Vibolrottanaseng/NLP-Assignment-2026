{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e550dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b3a53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b288335",
   "metadata": {},
   "source": [
    "# 1. Data Acquisition (Task 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66acdf9",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7533d720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to data/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"gutenberg\", download_dir=\"data/nltk_data\")\n",
    "import nltk\n",
    "nltk.data.path.append(\"data/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b7d0474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887071 [Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died t\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "text = gutenberg.raw(\"austen-emma.txt\")\n",
    "print(len(text), text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6884f5f",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de671cc",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "586d0dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraphs: 2371\n",
      "The evil of the actual disparity in their ages (and Mr. Woodhouse had\n",
      "not married early) was much increased by his constitution and habits;\n",
      "for having been a valetudinarian all his life, without activ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1896, 237, 238)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split into paragraphs\n",
    "paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "print(\"paragraphs:\", len(paras))\n",
    "print(paras[10][:200])\n",
    "\n",
    "# Split 90/5/5\n",
    "# n = len(paras)\n",
    "# train_paras = paras[: int(0.90*n)]\n",
    "# valid_paras = paras[int(0.90*n) : int(0.95*n)]\n",
    "# test_paras  = paras[int(0.95*n) :]\n",
    "\n",
    "# Split 80/10/10\n",
    "n = len(paras)\n",
    "train_paras = paras[: int(0.80 * n)]\n",
    "valid_paras = paras[int(0.80 * n) : int(0.90 * n)]\n",
    "test_paras  = paras[int(0.90 * n) :]\n",
    "\n",
    "train_dataset = [{\"text\": p} for p in train_paras]\n",
    "valid_dataset = [{\"text\": p} for p in valid_paras]\n",
    "test_dataset  = [{\"text\": p} for p in test_paras]\n",
    "\n",
    "len(train_dataset), len(valid_dataset), len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c525af9",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6296bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_re = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?|[0-9]+|[^\\sA-Za-z0-9]\")\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return token_re.findall(text.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3803dbf4",
   "metadata": {},
   "source": [
    "## Nummericalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a57c7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 2939\n",
      "itos[:20]: ['<unk>', '<pad>', '<eos>', ',', '.', '-', 'the', 'to', 'and', 'of', '\"', 'i', 'a', 'it', ';', 'was', 'her', 'she', 'not', 'in']\n"
     ]
    }
   ],
   "source": [
    "special = [\"<unk>\", \"<pad>\", \"<eos>\"]\n",
    "\n",
    "def build_vocab(paragraphs, min_freq=3):\n",
    "    counter = Counter()\n",
    "    for p in paragraphs:\n",
    "        counter.update(tokenize(p))\n",
    "\n",
    "    itos = list(special)\n",
    "    for tok, freq in counter.most_common():\n",
    "        if freq >= min_freq and tok not in special:\n",
    "            itos.append(tok)\n",
    "\n",
    "    stoi = {tok:i for i,tok in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "stoi, itos = build_vocab(train_paras, min_freq=3)\n",
    "vocab_size = len(itos)\n",
    "\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "print(\"itos[:20]:\", itos[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36faec4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([155786]), torch.Size([23013]), torch.Size([21859]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNK = stoi[\"<unk>\"]\n",
    "EOS = stoi[\"<eos>\"]\n",
    "\n",
    "def encode_paragraphs(paragraphs):\n",
    "    ids = []\n",
    "    for p in paragraphs:\n",
    "        toks = tokenize(p) + [\"<eos>\"]\n",
    "        ids.extend([stoi.get(t, UNK) for t in toks])\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "train_ids = encode_paragraphs(train_paras)\n",
    "valid_ids = encode_paragraphs(valid_paras)\n",
    "test_ids  = encode_paragraphs(test_paras)\n",
    "\n",
    "train_ids.shape, valid_ids.shape, test_ids.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ab53e",
   "metadata": {},
   "source": [
    "# 3. Prepare the batch loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7f1f6c",
   "metadata": {},
   "source": [
    "## Prepare ddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9df6a5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 2434]), torch.Size([64, 359]), torch.Size([64, 341]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data(data, batch_size):\n",
    "    n_batches = data.size(0) // batch_size\n",
    "    data = data[: n_batches * batch_size]\n",
    "    return data.view(batch_size, n_batches)\n",
    "\n",
    "batch_size = 64\n",
    "train_data = get_data(train_ids, batch_size)\n",
    "valid_data = get_data(valid_ids, batch_size)\n",
    "test_data  = get_data(test_ids,  batch_size)\n",
    "\n",
    "train_data.shape, valid_data.shape, test_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4bdc743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    x = data[:, idx:idx+seq_len]             # [B, S]\n",
    "    y = data[:, idx+1:idx+seq_len+1]         # [B, S]\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9640315",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "654661bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim, hid_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        h = torch.zeros(self.num_layers, batch_size, self.hid_dim, device=device)\n",
    "        c = torch.zeros(self.num_layers, batch_size, self.hid_dim, device=device)\n",
    "        return (h, c)\n",
    "\n",
    "    def detach_hidden(self, hidden):\n",
    "        return (hidden[0].detach(), hidden[1].detach())\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x: [B, S]\n",
    "        x = self.drop(self.emb(x))               # [B, S, E]\n",
    "        out, hidden = self.lstm(x, hidden)       # [B, S, H]\n",
    "        out = self.drop(out)\n",
    "        logits = self.fc(out)                    # [B, S, V]\n",
    "        return logits, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3ad4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data, optimizer, criterion, seq_len, clip):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    T = data.size(1)\n",
    "\n",
    "    hidden = model.init_hidden(batch_size=data.size(0), device=data.device)\n",
    "\n",
    "    for idx in tqdm(range(0, T - seq_len - 1, seq_len), desc=\"train\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        x, y = get_batch(data, seq_len, idx)\n",
    "        logits, hidden = model(x, hidden)\n",
    "\n",
    "        # flatten\n",
    "        B, S, V = logits.shape\n",
    "        loss = criterion(logits.reshape(B*S, V), y.reshape(B*S))\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / max(1, (T // seq_len))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, data, criterion, seq_len):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    T = data.size(1)\n",
    "\n",
    "    hidden = model.init_hidden(batch_size=data.size(0), device=data.device)\n",
    "\n",
    "    for idx in range(0, T - seq_len - 1, seq_len):\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "        x, y = get_batch(data, seq_len, idx)\n",
    "        logits, hidden = model(x, hidden)\n",
    "\n",
    "        B, S, V = logits.shape\n",
    "        loss = criterion(logits.reshape(B*S, V), y.reshape(B*S))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / max(1, (T // seq_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbd22efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Perplexity 294.472 | Valid Perplexity 135.579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | Train Perplexity 134.846 | Valid Perplexity 92.210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | Train Perplexity 101.764 | Valid Perplexity 77.885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | Train Perplexity 87.037 | Valid Perplexity 71.280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | Train Perplexity 78.249 | Valid Perplexity 67.169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | Train Perplexity 72.003 | Valid Perplexity 64.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | Train Perplexity 67.152 | Valid Perplexity 61.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | Train Perplexity 63.368 | Valid Perplexity 60.367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | Train Perplexity 59.929 | Valid Perplexity 59.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Perplexity 57.252 | Valid Perplexity 58.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Perplexity 54.626 | Valid Perplexity 57.063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Perplexity 52.387 | Valid Perplexity 56.989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Perplexity 50.243 | Valid Perplexity 56.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Perplexity 48.452 | Valid Perplexity 55.920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Perplexity 46.668 | Valid Perplexity 55.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train Perplexity 45.144 | Valid Perplexity 55.422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Train Perplexity 43.471 | Valid Perplexity 55.325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train Perplexity 42.054 | Valid Perplexity 55.374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Train Perplexity 40.762 | Valid Perplexity 55.338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Train Perplexity 39.506 | Valid Perplexity 55.246\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 1024\n",
    "hid_dim = 1024\n",
    "num_layers = 2\n",
    "dropout_rate = 0.65\n",
    "lr = 1e-3\n",
    "\n",
    "model = LSTMLM(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "seq_len = 35\n",
    "clip = 0.25\n",
    "epochs = 20\n",
    "\n",
    "best_valid = float(\"inf\")\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    tr_loss = train_epoch(model, train_data, optimizer, criterion, seq_len, clip)\n",
    "    va_loss = eval_epoch(model, valid_data, criterion, seq_len)\n",
    "\n",
    "    if va_loss < best_valid:\n",
    "        best_valid = va_loss\n",
    "        torch.save(model.state_dict(), \"model/language_model.pt\")\n",
    "\n",
    "    print(f\"Epoch {ep:02d} | Train Perplexity {math.exp(tr_loss):.3f} | Valid Perplexity {math.exp(va_loss):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4e41c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and vocabulary.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "with open(\"model/vocab_itos.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(itos, f, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved model and vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b4480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = {\n",
    "    \"emb_dim\": emb_dim,\n",
    "    \"hid_dim\": hid_dim,\n",
    "    \"num_layers\": num_layers\n",
    "}\n",
    "\n",
    "with open(\"model/config.json\", \"w\") as f:\n",
    "    json.dump(config, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "674b7667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 55.899205435391465\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model/language_model.pt\", map_location=device))\n",
    "te_loss = eval_epoch(model, test_data, criterion, seq_len)\n",
    "print(\"Test Perplexity:\", math.exp(te_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b0a7cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "UNK = stoi[\"<unk>\"]\n",
    "EOS = stoi[\"<eos>\"]\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(\n",
    "    model,\n",
    "    prompt,\n",
    "    max_new_tokens=60,\n",
    "    temperature=0.8,\n",
    "    seed=None\n",
    "):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenize(prompt.lower())\n",
    "    if not tokens:\n",
    "        tokens = [\"the\"]\n",
    "\n",
    "    ids = [stoi.get(t, UNK) for t in tokens]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size=1, device=device)\n",
    "\n",
    "    # warm up model with the prompt\n",
    "    x = torch.tensor([ids], dtype=torch.long, device=device)\n",
    "    _, hidden = model(x, hidden)\n",
    "\n",
    "    current_id = ids[-1]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        x = torch.tensor([[current_id]], dtype=torch.long, device=device)\n",
    "        logits, hidden = model(x, hidden)\n",
    "\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        next_id = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        if next_id == EOS:\n",
    "            break\n",
    "\n",
    "        ids.append(next_id)\n",
    "        current_id = next_id\n",
    "\n",
    "    return \" \".join(itos[i] for i in ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cbd2c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature = 0.5\n",
      "emma was pleased by the young lady .\n",
      "\n",
      "Temperature = 0.7\n",
      "emma was pleased into the mouth ; and when the <unk> of the churchills , as one of the happiness , was not often <unk> ; mr . knightley was telling her\n",
      "\n",
      "Temperature = 0.9\n",
      "emma was pleased into the mouth ; and when the <unk> of the parish , as one of the happiness , was not often <unk> ; mr . knightley must be safely\n",
      "\n",
      "Temperature = 1.0\n",
      "emma was pleased into the mouth ; and when the <unk> of the parish , as one of the happiness , was correct by no means mr . knightley , which seemed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"emma was \"\n",
    "max_new_tokens = 30\n",
    "seed = 0\n",
    "\n",
    "temperatures = [0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "for temperature in temperatures:\n",
    "    text = generate_text(\n",
    "        model,\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    print(f\"Temperature = {temperature}\")\n",
    "    print(text)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444cf6fc",
   "metadata": {},
   "source": [
    "Conclusion: <br>\n",
    "The LSTMs is train using a word level "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
